{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secrets\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from openrouteservice import client\n",
    "import shapely\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import sqlite3\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Vars\n",
    "CENSUS_KEY = secrets.CENSUS_API_KEY\n",
    "ORS_KEY = secrets.ORS_API_KEY\n",
    "\n",
    "CACHE_PATH = './Cache/Cache.json'\n",
    "CACHE_VAR = {}\n",
    "\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter?\"\n",
    "overpass_query_markets = '''[out:json]\n",
    "[timeout:25]\n",
    ";\n",
    "area(3600175905)->.searchArea;\n",
    "(\n",
    "  node\n",
    "    [\"shop\"=\"supermarket\"]\n",
    "    (area.searchArea);\n",
    "  way\n",
    "    [\"shop\"=\"supermarket\"]\n",
    "    (area.searchArea);\n",
    "  relation\n",
    "    [\"shop\"=\"supermarket\"]\n",
    "    (area.searchArea);\n",
    "  node\n",
    "    [\"shop\"=\"grocery\"]\n",
    "    (area.searchArea);\n",
    "  way\n",
    "    [\"shop\"=\"grocery\"]\n",
    "    (area.searchArea);\n",
    "  relation\n",
    "    [\"shop\"=\"grocery\"]\n",
    "    (area.searchArea);\n",
    "  node\n",
    "    [\"shop\"=\"greengrocer\"]\n",
    "    (area.searchArea);\n",
    "  way\n",
    "    [\"shop\"=\"greengrocer\"]\n",
    "    (area.searchArea);\n",
    "  relation\n",
    "    [\"shop\"=\"greengrocer\"]\n",
    "    (area.searchArea);\n",
    ");\n",
    "out center;\n",
    ">;\n",
    "out skel qt;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_cache(cache_path):\n",
    "    '''\n",
    "        Opens the cache with the file path provided as a dictionary; if no cache is present,\n",
    "        creates a cache dictionary.\n",
    "\n",
    "        Returns the resultant cache dictionary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cache_path: str\n",
    "            The path to a cache file, if such a file exists.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary containing cached information stored in the form of a json.\n",
    "        '''\n",
    "    try:\n",
    "        with open(cache_path, 'r') as cache_file:\n",
    "                cache = json.load(cache_file_contents)\n",
    "    except:\n",
    "        cache = {}\n",
    "    \n",
    "    return cache\n",
    "\n",
    "\n",
    "def save_cache(cache_data, cache_name):\n",
    "    '''\n",
    "        Saves a cache dictionary to the filepath provided.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cache_name: dict\n",
    "            A dictionary containing cached webpage information.\n",
    "        \n",
    "        cache_path: str\n",
    "            The file path where the cache is to be saved.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "    if cache_name is not None and CACHE_VAR is not None:\n",
    "        updated_cache = CACHE_VAR.copy()\n",
    "        updated_cache[cache_name] = cache_data\n",
    "    else:\n",
    "        updated_cache = cache_data\n",
    "    \n",
    "    with open(CACHE_PATH, 'w') as cache_file:\n",
    "        json.dump(updated_cache, cache_file, indent=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_unique_key(params, api_url):\n",
    "    '''\n",
    "        Constructs a unique key for a webpage (to be used in this program's\n",
    "        cache) from supplied parameters and an API URL.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params: dict\n",
    "            A dictionary containing search parameters.\n",
    "\n",
    "        api_url: string\n",
    "            The location of an API. Defaults to global API_URL.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A unique key.\n",
    "        '''\n",
    "    param_strings = []\n",
    "    connector = '_'\n",
    "    for k in params.keys():\n",
    "        param_strings.append(f'{k}_{params[k]}')\n",
    "    unique_key = api_url + connector + connector.join(param_strings)\n",
    "    return unique_key\n",
    "\n",
    "\n",
    "def call_API_with_cache(url, params, cache_name, reset_cache=False):\n",
    "    '''\n",
    "        Manages API calls using the provided cache of API-derived data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        url: string\n",
    "            The location of a resource to be requested.\n",
    "\n",
    "        params: dict\n",
    "            A dictionary containing search parameters; defaults to None.\n",
    "            \n",
    "        cache: dict\n",
    "            A dictionary containing previously obtained data.\n",
    "            \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            The JSON returned by the API call, formatted as a dictionary.\n",
    "        '''\n",
    "\n",
    "    if reset_cache == False:\n",
    "        temp_cache = CACHE_VAR.setdefault(cache_name, {})\n",
    "    else:\n",
    "        temp_cache = {}\n",
    "    \n",
    "    if params is not None:\n",
    "        key = construct_unique_key(params, url)\n",
    "    else:\n",
    "        key = url\n",
    "        \n",
    "    if key in temp_cache.keys():\n",
    "        print(f\"Using Cache: {url}\")\n",
    "        content = temp_cache[key]\n",
    "        return content\n",
    "    else:\n",
    "        print(f\"Fetching: {url}\")\n",
    "        if params is not None:\n",
    "            content = requests.get(url=url, params=params).json()\n",
    "        else:\n",
    "            content = requests.get(url=url).json()\n",
    "            \n",
    "        temp_cache[key] = content\n",
    "        save_cache(temp_cache, cache_name)\n",
    "            \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_markets(refresh=False):\n",
    "    '''TODO: Docstring\n",
    "    \n",
    "    Fetches market data, saves and returns as GeoJSON'''\n",
    "    \n",
    "    geographic_elements = {'type':'FeatureCollection',\n",
    "                      'name':'markets',\n",
    "                      'features':[]}\n",
    "    \n",
    "    \n",
    "    results = call_API_with_cache(url=overpass_url, \n",
    "                                  params={'data':overpass_query_markets}, \n",
    "                                  cache_name='markets', reset_cache=refresh)\n",
    "    \n",
    "    for element in results['elements']:\n",
    "        if 'tags' in element:\n",
    "            geodict = {'type':'Point'}\n",
    "            propdict = {'id':element['id']}\n",
    "\n",
    "            if element['type'] == 'node' and 'tags' in element:\n",
    "                lon = element['lon']\n",
    "                lat = element['lat']\n",
    "                geodict['coordinates'] = [lon, lat]\n",
    "\n",
    "            elif 'center' in element:\n",
    "                lon = element['center']['lon']\n",
    "                lat = element['center']['lat']\n",
    "                geodict['coordinates'] = [lon, lat]\n",
    "\n",
    "            for key, value in element['tags'].items():\n",
    "                propdict[key] = value\n",
    "\n",
    "            geographic_elements['features'].append({'type':'Feature',\n",
    "                                       'geometry':geodict,\n",
    "                                       'properties':propdict})\n",
    "\n",
    "    with open('Geospatial_Data/markets.geojson', 'w') as file:\n",
    "        json.dump(geographic_elements, file, indent=2)\n",
    "        \n",
    "    return geographic_elements\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_features(feature_df, n, geometry_col, id_col):\n",
    "    '''TODO: Write Docstring'''\n",
    "    ids_with_locations = {}\n",
    "\n",
    "    feature_df['geom_reformat'] = feature_df[geometry_col].apply(lambda location: [location.x, location.y])\n",
    "    \n",
    "    df_chunks = np.array_split(feature_df[[id_col, 'geom_reformat']], math.trunc(feature_df.shape[0]/5)+1)\n",
    "    \n",
    "    for chunk in df_chunks:\n",
    "        id_string = ''\n",
    "        \n",
    "        for item in chunk[id_col].tolist():\n",
    "            id_string += f'{item}_'\n",
    "        \n",
    "        location_list = chunk['geom_reformat'].tolist()\n",
    "        \n",
    "        ids_with_locations[id_string] = location_list\n",
    "        \n",
    "\n",
    "    return ids_with_locations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_isochrones(points):\n",
    "    '''TODO: Docstring\n",
    "    \n",
    "    Returns dictionary containing an index and a list of GeoJSON Features'''\n",
    "    points_data = gpd.read_file(json.dumps(points))\n",
    "    \n",
    "    segments = divide_features(points_data, 5, 'geometry', 'id')\n",
    "\n",
    "    params = {'location_type':'destination',\n",
    "              'range': [600, 420, 300], #420/60 = 7 mins\n",
    "              'range_type': 'time',\n",
    "              'attributes': ['area', 'reachfactor', 'total_pop'], # Get attributes for isochrones\n",
    "              'smoothing': 5\n",
    "             }\n",
    "\n",
    "    header = {\n",
    "        'Accept': 'application/json, application/geo+json, application/gpx+xml, img/png; charset=utf-8',\n",
    "        'Authorization': ORS_KEY,\n",
    "        'Content-Type': 'application/json; charset=utf-8'\n",
    "    }\n",
    "\n",
    "    isochrone_features = []\n",
    "    index = []\n",
    "\n",
    "    for id_string, locations in segments.items():\n",
    "        params['locations'] = locations\n",
    "        params['id'] = id_string\n",
    "        \n",
    "        id_list = np.repeat(id_string.split(sep='_'), len(params['range']))\n",
    "        index += id_list\n",
    "\n",
    "        try:\n",
    "            isos = requests.post(ORS_URL, json=params, headers=header).json()\n",
    "\n",
    "            i = 0\n",
    "            for feature in isos['features']:\n",
    "                feature['properties']['id'] = id_list[i]\n",
    "                i += 1\n",
    "                isochrone_features.append(feature)\n",
    "\n",
    "        except:\n",
    "            time.sleep(61)\n",
    "\n",
    "            isos = requests.post(ORS_URL, json=params_iso, headers=head_iso).json()\n",
    "\n",
    "            i = 0\n",
    "            for feature in isos['features']:\n",
    "                feature['properties']['id'] = id_list[i]\n",
    "                i += 1\n",
    "                isochrone_features.append(feature)\n",
    "\n",
    "    return {'index': index, 'features': isochrone_features}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_isochrones_with_cache(point_feature_collection):\n",
    "    '''TODO: Docstring\n",
    "    \n",
    "    point_feature_collection: GeoJSON,\n",
    "    returns GeoJSON of '''\n",
    "    \n",
    "    iso_cache = CACHE_VAR.setdefault('isochrones',{'index':[],\n",
    "                                                   'GeoJSON':{'type': 'FeatureCollection',\n",
    "                                                             'name': 'isochrones',\n",
    "                                                             'features':[]\n",
    "                                                             }\n",
    "                                                  })\n",
    "    \n",
    "    features_in_cache = []\n",
    "    features_to_fetch = []\n",
    "    \n",
    "    for feature in point_feature_collection['features']:\n",
    "        if feature['properties']['id'] in iso_cache['index']:\n",
    "            features_in_cache.append(feature)\n",
    "        else:\n",
    "            features_to_fetch.append(feature)\n",
    "    \n",
    "    new_isochrones = get_isochrones({'type': 'FeatureCollection',\n",
    "                                     'name': 'temp'\n",
    "                                     'features': features_to_fetch})\n",
    "    \n",
    "    iso_cache['index'] += new_isochrones['index']\n",
    "    iso_cache['GeoJSON']['features'] += iso_cache['features']\n",
    "    \n",
    "    save_cache(iso_cache, 'isochrones')\n",
    "    \n",
    "    with open('Geospatial_Data/isochrones.geojson', 'w') as file:\n",
    "        json.dump(geographic_elements, file, indent=2)\n",
    "\n",
    "    return iso_cache['GeoJSON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: http://overpass-api.de/api/interpreter?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-05d30f0b4c7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#Fetch and Categorize Isochrones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0misochrones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_isochrones_with_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-2f26734de835>\u001b[0m in \u001b[0;36mget_isochrones_with_cache\u001b[0;34m(point_feature_collection)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     new_isochrones = get_isochrones({'type': 'FeatureCollection',\n\u001b[0;32m---> 24\u001b[0;31m                                      'features': features_to_fetch})\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0miso_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnew_isochrones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-6e4173a4f937>\u001b[0m in \u001b[0;36mget_isochrones\u001b[0;34m(points)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpoints_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivide_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'geometry'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     params = {'location_type':'destination',\n",
      "\u001b[0;32m<ipython-input-6-c09f92455cf6>\u001b[0m in \u001b[0;36mdivide_features\u001b[0;34m(feature_df, n, geometry_col, id_col)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfeature_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'geom_reformat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgeometry_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdf_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'geom_reformat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_chunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#if __name__ == \"main\":\n",
    "    #Initialize Cache\n",
    "CACHE_VAR = open_cache(CACHE_PATH)\n",
    "    \n",
    "    #Fetch Markets\n",
    "markets = get_markets()\n",
    "    \n",
    "    #Fetch and Categorize Isochrones\n",
    "isochrones = get_isochrones_with_cache(markets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
