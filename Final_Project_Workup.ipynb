{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secrets\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from openrouteservice import client\n",
    "import shapely\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import sqlite3\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'secrets' has no attribute 'CENSUS_API_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-87c31430a739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Global Vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mCENSUS_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCENSUS_API_KEY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mORS_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mORS_API_KEY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moverpass_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://overpass-api.de/api/interpreter?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'secrets' has no attribute 'CENSUS_API_KEY'"
     ]
    }
   ],
   "source": [
    "#Global Vars\n",
    "CENSUS_KEY = secrets.CENSUS_API_KEY\n",
    "ORS_KEY = secrets.ORS_API_KEY\n",
    "\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter?\"\n",
    "overpass_query_markets = '''[out:json]\n",
    "[timeout:25]\n",
    ";\n",
    "area(3600175905)->.searchArea;\n",
    "(\n",
    "  node\n",
    "    [\"shop\"=\"supermarket\"]\n",
    "    (area.searchArea);\n",
    "  way\n",
    "    [\"shop\"=\"supermarket\"]\n",
    "    (area.searchArea);\n",
    "  relation\n",
    "    [\"shop\"=\"supermarket\"]\n",
    "    (area.searchArea);\n",
    "  node\n",
    "    [\"shop\"=\"grocery\"]\n",
    "    (area.searchArea);\n",
    "  way\n",
    "    [\"shop\"=\"grocery\"]\n",
    "    (area.searchArea);\n",
    "  relation\n",
    "    [\"shop\"=\"grocery\"]\n",
    "    (area.searchArea);\n",
    "  node\n",
    "    [\"shop\"=\"greengrocer\"]\n",
    "    (area.searchArea);\n",
    "  way\n",
    "    [\"shop\"=\"greengrocer\"]\n",
    "    (area.searchArea);\n",
    "  relation\n",
    "    [\"shop\"=\"greengrocer\"]\n",
    "    (area.searchArea);\n",
    ");\n",
    "out center;\n",
    ">;\n",
    "out skel qt;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_cache(cache_path=None):\n",
    "    '''\n",
    "        Opens the cache with the file path provided as a dictionary; if no cache is present,\n",
    "        creates a cache dictionary.\n",
    "\n",
    "        Returns the resultant cache dictionary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cache_path: str\n",
    "            The path to a cache file, if such a file exists.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary containing cached information stored in the form of a json.\n",
    "        '''\n",
    "    try:\n",
    "         with open(cache_path, 'r') as cache_file:\n",
    "                cache = json.load(cache_file_contents)\n",
    "    except:\n",
    "        cache = {}\n",
    "    return cache\n",
    "\n",
    "\n",
    "def save_cache(cache, cache_path):\n",
    "    '''\n",
    "        Saves a cache dictionary to the filepath provided.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cache: dict\n",
    "            A dictionary containing cached webpage information.\n",
    "        \n",
    "        cache_path: str\n",
    "            The file path where the cache is to be saved.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "      with open(cache_path, 'w') as cache_file:\n",
    "            json.dump(cache, cache_file, indent=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_API_with_cache(url, params=None, cache=None):\n",
    "    '''\n",
    "        Manages API calls using the provided cache of API-derived data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        url: string\n",
    "            The location of a resource to be requested.\n",
    "\n",
    "        params: dict\n",
    "            A dictionary containing search parameters; defaults to None.\n",
    "            \n",
    "        cache: dict\n",
    "            A dictionary containing previously obtained data.\n",
    "            \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            The JSON returned by the API call, formatted as a dictionary.\n",
    "        '''\n",
    "    if params is not None:\n",
    "        key = construct_unique_key(params, url)\n",
    "    else:\n",
    "        key = url\n",
    "        \n",
    "    if cach is not None and key in cache.keys():\n",
    "        print(f\"Using Cache: {url}\")\n",
    "        content = cache[key]\n",
    "        return content\n",
    "    else:\n",
    "        print(f\"Fetching: {url}\")\n",
    "        if params is not None:\n",
    "            content = requests.get(url=url, params=params).json()\n",
    "        else:\n",
    "            content = requests.get(url=url).json()\n",
    "            \n",
    "        cache[key] = content\n",
    "        save_cache(cache)\n",
    "            \n",
    "    return content\n",
    "\n",
    "def construct_unique_key(params, api_url=API_URL):\n",
    "    '''\n",
    "        Constructs a unique key for a webpage (to be used in this program's\n",
    "        cache) from supplied parameters and an API URL.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params: dict\n",
    "            A dictionary containing search parameters.\n",
    "\n",
    "        api_url: string\n",
    "            The location of an API. Defaults to global API_URL.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A unique key.\n",
    "        '''\n",
    "    param_strings = []\n",
    "    connector = '_'\n",
    "    for k in params.keys():\n",
    "        param_strings.append(f'{k}_{params[k]}')\n",
    "    unique_key = api_url + connector + connector.join(param_strings)\n",
    "    return unique_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-766621b39913>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-766621b39913>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    if refresh = True:\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_markets(refresh=False):\n",
    "    '''TODO: Docstring\n",
    "    \n",
    "    Fetches market data, saves and returns as GeoJSON'''\n",
    "    \n",
    "    geographic_elements = {'type':'FeatureCollection',\n",
    "                      'name':'markets',\n",
    "                      'features':[]}\n",
    "    \n",
    "    if refresh = True:\n",
    "        market_cache = None\n",
    "    else:\n",
    "        market_cache = open_cache(MARKET_CACHE_PATH)\n",
    "    \n",
    "    results = call_API_with_cache(url=overpass_url, params={'data':overpass_query_markets}, cache=market_cache)\n",
    "    \n",
    "    for element in results['elements']:\n",
    "    if 'tags' in element:\n",
    "        geodict = {'type':'Point'}\n",
    "        propdict = {'id':element['id']}\n",
    "\n",
    "        if element['type'] == 'node' and 'tags' in element:\n",
    "            lon = element['lon']\n",
    "            lat = element['lat']\n",
    "            geodict['coordinates'] = [lon, lat]\n",
    "            \n",
    "        elif 'center' in element:\n",
    "            lon = element['center']['lon']\n",
    "            lat = element['center']['lat']\n",
    "            geodict['coordinates'] = [lon, lat]\n",
    "    \n",
    "        for key, value in element['tags'].items():\n",
    "            propdict[key] = value\n",
    "        \n",
    "        geographic_elements['features'].append({'type':'Feature',\n",
    "                                   'geometry':geodict,\n",
    "                                   'properties':propdict})\n",
    "\n",
    "    with open('./markets.geojson', 'w') as file:\n",
    "        json.dump(geographic_elements, file, indent=2)\n",
    "    \n",
    "    return geographic_elements\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_features(feature_df, n, geometry_col, id_col):\n",
    "    '''TODO: Write Docstring'''\n",
    "    ids_with_locations = {}\n",
    "\n",
    "    feature_df['geom_reformat'] = feature_df[geometry_col].apply(lambda location: [location.x, location.y])\n",
    "    \n",
    "    df_chunks = np.array_split(feature_df[[id_col, 'geom_reformat']], math.trunc(feature_df.shape[0]/5)+1)\n",
    "    \n",
    "    for chunk in df_chunks:\n",
    "        id_string = ''\n",
    "        \n",
    "        for item in chunk[id_col].tolist():\n",
    "            id_string += f'{item}_'\n",
    "        \n",
    "        location_list = chunk['geom_reformat'].tolist()\n",
    "        \n",
    "        ids_with_locations[id_string] = location_list\n",
    "        \n",
    "\n",
    "    return ids_with_locations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_isochrones(points):\n",
    "    points_data = gpd.GeoDataFrame(points)\n",
    "    clnt = client.Client(key=ORS_KEY)\n",
    "    \n",
    "    segments = divide_features(markets, 5, 'geometry', 'id')\n",
    "\n",
    "    params_iso = {'location_type':'destination',\n",
    "              'range': [600, 420, 300], #420/60 = 7 mins\n",
    "              'range_type': 'time',\n",
    "              'attributes': ['area', 'reachfactor', 'total_pop'], # Get attributes for isochrones\n",
    "              'smoothing': 5\n",
    "             }\n",
    "\n",
    "    head_iso = {\n",
    "        'Accept': 'application/json, application/geo+json, application/gpx+xml, img/png; charset=utf-8',\n",
    "        'Authorization': ORS_KEY,\n",
    "        'Content-Type': 'application/json; charset=utf-8'\n",
    "    }\n",
    "\n",
    "    isochrones = {'type':'FeatureCollection',\n",
    "                     'features':[]}\n",
    "    \n",
    "    index = []\n",
    "\n",
    "    for id_string, locations in segments.items():\n",
    "        params_iso['locations'] = locations\n",
    "        params_iso['id'] = id_string\n",
    "        \n",
    "        id_list = np.repeat(id_string.split(sep='_'), len(params_iso['range']))\n",
    "        index += id_list\n",
    "\n",
    "        try:\n",
    "            isos = requests.post(ORS_URL, json=params_iso, headers=head_iso).json()\n",
    "\n",
    "            i = 0\n",
    "            for feature in isos['features']:\n",
    "                feature['properties']['id'] = id_list[i]\n",
    "                i += 1\n",
    "                isochrones['features'].append(feature)\n",
    "                print(feature)\n",
    "\n",
    "        except:\n",
    "            time.sleep(61)\n",
    "\n",
    "            isos = requests.post(ORS_URL, json=params_iso, headers=head_iso).json()\n",
    "\n",
    "            i = 0\n",
    "            for feature in isos['features']:\n",
    "                feature['properties']['id'] = id_list[i]\n",
    "                i += 1\n",
    "                isochrones['features'].append(feature)\n",
    "\n",
    "    return {'index': index, 'isochrones': isochrones}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_isochrones_with_cache(market_geographies):\n",
    "    #SET UP TO WORK WITH CACHE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR 507 FINAL\n",
    "def open_cache(cache_name, cache_path):\n",
    "    '''TODO: Docstring'''\n",
    "    with open(cache_path) as cache:\n",
    "        cache_name = json.load(cache)\n",
    "    \n",
    "    return cache_name\n",
    "\n",
    "def save_cache(cache_name, cache_path):\n",
    "    '''TODO: Docstring'''\n",
    "    with open(cache_path, 'w') as file:\n",
    "        json.dump(cache_name, file, indent=2)\n",
    "\n",
    "def make_market_database(url=overpass_url, query=overpass_query_markets):\n",
    "    '''TODO: Docstring'''\n",
    "    \n",
    "    results = requests.get(url=url, params={'data':query}).json()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-2-64dc2eee03d0>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-64dc2eee03d0>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    markets = get_markets_with_cache(overpass_url, overpass_query_markets)'\u001b[0m\n\u001b[0m                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"main\":\n",
    "    \n",
    "    #Get markets\n",
    "    markets = gpd.get_markets()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
