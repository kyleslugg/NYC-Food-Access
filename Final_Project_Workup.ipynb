{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secrets\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from openrouteservice import client\n",
    "import shapely\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import sqlite3\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CENSUS_KEY = secrets.CENSUS_API_KEY\n",
    "\n",
    "\n",
    "ORS_KEY = secrets.ORS_API_KEY\n",
    "ORS_URL = 'https://api.openrouteservice.org/v2/isochrones/foot-walking'\n",
    "\n",
    "CACHE_PATH = './Cache/Cache.json'\n",
    "CACHE_VAR = {}\n",
    "\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter?\"\n",
    "overpass_query_markets = '''[out:json]\n",
    "[timeout:25]\n",
    ";\n",
    "area(3600175905)->.searchArea;\n",
    "(\n",
    "  node\n",
    "    [\"shop\"=\"supermarket\"]\n",
    "    (area.searchArea);\n",
    "  way\n",
    "    [\"shop\"=\"supermarket\"]\n",
    "    (area.searchArea);\n",
    "  relation\n",
    "    [\"shop\"=\"supermarket\"]\n",
    "    (area.searchArea);\n",
    "  node\n",
    "    [\"shop\"=\"grocery\"]\n",
    "    (area.searchArea);\n",
    "  way\n",
    "    [\"shop\"=\"grocery\"]\n",
    "    (area.searchArea);\n",
    "  relation\n",
    "    [\"shop\"=\"grocery\"]\n",
    "    (area.searchArea);\n",
    "  node\n",
    "    [\"shop\"=\"greengrocer\"]\n",
    "    (area.searchArea);\n",
    "  way\n",
    "    [\"shop\"=\"greengrocer\"]\n",
    "    (area.searchArea);\n",
    "  relation\n",
    "    [\"shop\"=\"greengrocer\"]\n",
    "    (area.searchArea);\n",
    ");\n",
    "out center;\n",
    ">;\n",
    "out skel qt;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_cache(cache_path):\n",
    "    '''\n",
    "        Opens the cache with the file path provided as a dictionary; if no cache is present,\n",
    "        creates a cache dictionary.\n",
    "\n",
    "        Returns the resultant cache dictionary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cache_path: str\n",
    "            The path to a cache file, if such a file exists.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary containing cached information stored in the form of a json.\n",
    "        '''\n",
    "    try:\n",
    "        with open(cache_path, 'r') as cache_file:\n",
    "                cache = json.load(cache_file)\n",
    "    except:\n",
    "        cache = {}\n",
    "\n",
    "    return cache\n",
    "\n",
    "\n",
    "def save_cache(cache_data, cache_name):\n",
    "    '''\n",
    "        Saves a cache dictionary to the filepath provided.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cache_name: dict\n",
    "            A dictionary containing cached webpage information.\n",
    "\n",
    "        cache_path: str\n",
    "            The file path where the cache is to be saved.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "    if cache_name is not None and CACHE_VAR is not None:\n",
    "        updated_cache = CACHE_VAR.copy()\n",
    "        updated_cache[cache_name] = cache_data\n",
    "    else:\n",
    "        updated_cache = cache_data\n",
    "\n",
    "    with open(CACHE_PATH, 'w') as cache_file:\n",
    "        json.dump(updated_cache, cache_file, indent=2)\n",
    "\n",
    "\n",
    "def construct_unique_key(params, api_url):\n",
    "    '''\n",
    "        Constructs a unique key for a webpage (to be used in this program's\n",
    "        cache) from supplied parameters and an API URL.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params: dict\n",
    "            A dictionary containing search parameters.\n",
    "\n",
    "        api_url: string\n",
    "            The location of an API. Defaults to global API_URL.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A unique key.\n",
    "        '''\n",
    "    param_strings = []\n",
    "    connector = '_'\n",
    "    for k in params.keys():\n",
    "        param_strings.append(f'{k}_{params[k]}')\n",
    "    unique_key = api_url + connector + connector.join(param_strings)\n",
    "    return unique_key\n",
    "\n",
    "\n",
    "def call_API_with_cache(url, params, cache_name, reset_cache=False):\n",
    "    '''\n",
    "        Manages API calls using the provided cache of API-derived data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        url: string\n",
    "            The location of a resource to be requested.\n",
    "\n",
    "        params: dict\n",
    "            A dictionary containing search parameters; defaults to None.\n",
    "\n",
    "        cache: dict\n",
    "            A dictionary containing previously obtained data.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            The JSON returned by the API call, formatted as a dictionary.\n",
    "        '''\n",
    "\n",
    "    if reset_cache == False:\n",
    "        temp_cache = CACHE_VAR.setdefault(cache_name, {})\n",
    "    else:\n",
    "        temp_cache = {}\n",
    "\n",
    "    if params is not None:\n",
    "        key = construct_unique_key(params, url)\n",
    "    else:\n",
    "        key = url\n",
    "\n",
    "    if key in temp_cache.keys():\n",
    "        print(f\"Using Cache: {url}\")\n",
    "        content = temp_cache[key]\n",
    "        return content\n",
    "    else:\n",
    "        print(f\"Fetching: {url}\")\n",
    "        if params is not None:\n",
    "            content = requests.get(url=url, params=params).json()\n",
    "        else:\n",
    "            content = requests.get(url=url).json()\n",
    "\n",
    "        temp_cache[key] = content\n",
    "        save_cache(temp_cache, cache_name)\n",
    "\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_market_data(refresh=False):\n",
    "    '''TODO: Docstring\n",
    "\n",
    "    Fetches market data, saves and returns as GeoJSON'''\n",
    "\n",
    "    geographic_elements = {'type':'FeatureCollection',\n",
    "                      'name':'markets',\n",
    "                      'features':[]}\n",
    "\n",
    "\n",
    "    results = call_API_with_cache(url=overpass_url,\n",
    "                                  params={'data':overpass_query_markets},\n",
    "                                  cache_name='markets', reset_cache=refresh)\n",
    "\n",
    "    for element in results['elements']:\n",
    "        if 'tags' in element:\n",
    "            geodict = {'type':'Point'}\n",
    "            propdict = {'id':element['id']}\n",
    "\n",
    "            if element['type'] == 'node' and 'tags' in element:\n",
    "                lon = element['lon']\n",
    "                lat = element['lat']\n",
    "                geodict['coordinates'] = [lon, lat]\n",
    "\n",
    "            elif 'center' in element:\n",
    "                lon = element['center']['lon']\n",
    "                lat = element['center']['lat']\n",
    "                geodict['coordinates'] = [lon, lat]\n",
    "\n",
    "            for key, value in element['tags'].items():\n",
    "                propdict[key] = value\n",
    "\n",
    "            geographic_elements['features'].append({'type':'Feature',\n",
    "                                       'geometry':geodict,\n",
    "                                       'properties':propdict})\n",
    "    \n",
    "    \n",
    "    markets_data = gpd.read_file(json.dumps(geographic_elements))\n",
    "    markets_data['wkb_geometry'] = markets_data['geometry'].apply(lambda item: item.wkb)\n",
    "    markets_data['addr'] = markets_data.apply(lambda row: f\"{row['addr:housenumber']} {row['addr:street']}, {row['addr:city']}\", axis=1)\n",
    "    markets_data['addr'] = markets_data['addr'].apply(lambda x: None if (str(x).find('None') != -1) else x)\n",
    "    \n",
    "    census_tracts = gpd.read_file('Geospatial_Data/NYC_Tracts.geojson').to_crs('epsg:4326')\n",
    "    fields_to_keep = ('id', 'name', 'alt_name', 'addr', 'shop', 'opening_hours', 'phone', 'GEOID','wkb_geometry')\n",
    "    \n",
    "    markets_data_with_tract = gpd.sjoin(markets_data, census_tracts, how='left', op='intersects')\n",
    "    markets_data_with_tract.drop(columns=[column for column in markets_data_with_tract.columns \n",
    "                                          if column not in fields_to_keep], inplace=True)\n",
    "\n",
    "                                                                    \n",
    "    #markets_data_with_tract.to_file('Geospatial_Data/markets.geojson', driver='GeoJSON')\n",
    "    \n",
    "    \n",
    "    #ADD DATABASE-WRITING FUNCTIONALITY\n",
    "    \n",
    "    make_markets_table(markets_data_with_tract)\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def make_markets_table(geodataframe):\n",
    "    '''TODO: Docstring\n",
    "    geom_column must be in wkb form'''\n",
    "    \n",
    "    conn = sqlite3.connect('Geospatial_Data/map_data.sqlite')\n",
    "    conn.enable_load_extension(True)\n",
    "    conn.load_extension(\"mod_spatialite\")\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT InitSpatialMetaData(1);\")\n",
    "    \n",
    "    #Generate Schema\n",
    "    create_statement = '''CREATE TABLE IF NOT EXISTS \"markets\"(\n",
    "    \"id\" INTEGER PRIMARY KEY UNIQUE,\n",
    "    \"name\" TEXT,\n",
    "    \"alt_name\" TEXT,\n",
    "    \"addr\" TEXT,\n",
    "    \"shop\" TEXT,\n",
    "    \"opening_hours\" TEXT,\n",
    "    \"phone\" TEXT,\n",
    "    \"GEOID\" TEXT NOT NULL)'''\n",
    "    \n",
    "    \n",
    "    drop_statement = f'''DROP TABLE IF EXISTS \"markets;\"'''\n",
    "    \n",
    "    cur.execute(drop_statement)\n",
    "    cur.execute(create_statement)\n",
    "    \n",
    "    def add_row(row):\n",
    "        add_statement = '''INSERT INTO markets\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)'''\n",
    "        values = row.values.tolist()[:8]\n",
    "        cur.execute(add_statement, values)\n",
    "    \n",
    "    geodataframe.apply(lambda row: add_row(row), axis=1)\n",
    "    \n",
    "    cur.execute(f\"\"\"\n",
    "        SELECT AddGeometryColumn(\"markets\", 'wkb_geometry', '{str(geodataframe.crs)[-4:]}, 'POINT', 2);\n",
    "        \"\"\")\n",
    "    geometry_tuples = []\n",
    "    geodataframe.apply(lambda row: geometry_tuples.append(tuple(row['wkb_geometry'], row['id'])))\n",
    "    \n",
    "    cur.executemany(\n",
    "    f\"\"\"\n",
    "    UPDATE markets\n",
    "    SET wkb_geometry=GeomFromWKB(?, {str(geodataframe.crs)[-4:]})\n",
    "    WHERE markets.id = ?\n",
    "    \"\"\", tuple(geometry_tuples))\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_features(feature_df, n, geometry_col, id_col):\n",
    "    '''TODO: Write Docstring'''\n",
    "    ids_with_locations = {}\n",
    "\n",
    "    feature_df['geom_reformat'] = feature_df[geometry_col].apply(lambda location: [location.x, location.y])\n",
    "\n",
    "    df_chunks = np.array_split(feature_df[[id_col, 'geom_reformat']], math.trunc(feature_df.shape[0]/5)+1)\n",
    "\n",
    "    for chunk in df_chunks:\n",
    "        id_string = ''\n",
    "\n",
    "        for item in chunk[id_col].tolist():\n",
    "            id_string += f'{item}_'\n",
    "\n",
    "        location_list = chunk['geom_reformat'].tolist()\n",
    "\n",
    "        ids_with_locations[id_string] = location_list\n",
    "\n",
    "\n",
    "    return ids_with_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_isochrones_with_cache(points, layer_cache):\n",
    "    '''TODO: Docstring\n",
    "\n",
    "    Returns dictionary containing an index and a list of GeoJSON Features'''\n",
    "\n",
    "    points_data = gpd.read_file(json.dumps(points))\n",
    "\n",
    "    segments = divide_features(points_data, 5, 'geometry', 'id')\n",
    "\n",
    "    params = {'location_type':'destination',\n",
    "              'range': [600, 420, 300], #420/60 = 7 mins\n",
    "              'range_type': 'time',\n",
    "              'attributes': ['area', 'reachfactor', 'total_pop'], # Get attributes for isochrones\n",
    "              'smoothing': 5\n",
    "             }\n",
    "\n",
    "    header = {\n",
    "        'Accept': 'application/json, application/geo+json, application/gpx+xml, img/png; charset=utf-8',\n",
    "        'Authorization': ORS_KEY,\n",
    "        'Content-Type': 'application/json; charset=utf-8'\n",
    "    }\n",
    "\n",
    "    isochrone_features = layer_cache['GeoJSON']['features']\n",
    "    index = layer_cache['index']\n",
    "\n",
    "    segment_number = 1\n",
    "    for id_string, locations in segments.items():\n",
    "        params['locations'] = locations\n",
    "        params['id'] = id_string\n",
    "        id_list = np.repeat(id_string.split(sep='_'), len(params['range'])).tolist()\n",
    "        index += id_list\n",
    "\n",
    "        try:\n",
    "            isos = requests.post(ORS_URL, json=params, headers=header).json()\n",
    "\n",
    "            i = 0\n",
    "            for feature in isos['features']:\n",
    "                feature['properties']['id'] = id_list[i]\n",
    "                i += 1\n",
    "                isochrone_features.append(feature)\n",
    "\n",
    "            save_cache(layer_cache, layer_cache['GeoJSON']['name'])\n",
    "            print(f\"Fetched New Isochrones: Segment {segment_number} of {len(segments.keys())}\")\n",
    "            segment_number +=1\n",
    "\n",
    "        except:\n",
    "            print(\"Waiting one minute...\")\n",
    "            time.sleep(61)\n",
    "\n",
    "            isos = requests.post(ORS_URL, json=params, headers=header).json()\n",
    "\n",
    "            i = 0\n",
    "            for feature in isos['features']:\n",
    "                feature['properties']['id'] = id_list[i]\n",
    "                i += 1\n",
    "                isochrone_features.append(feature)\n",
    "\n",
    "            save_cache(layer_cache, layer_cache['GeoJSON']['name'])\n",
    "            print(f\"Fetched New Isochrones: Segment {segment_number} of {len(segments.keys())}\")\n",
    "            segment_number +=1\n",
    "\n",
    "    return {'index': index, 'features': isochrone_features}\n",
    "\n",
    "\n",
    "def refresh_isochrones(point_feature_collection, layer_name):\n",
    "    '''TODO: Docstring\n",
    "\n",
    "    point_feature_collection: GeoJSON,\n",
    "    returns GeoJSON of '''\n",
    "\n",
    "    layer_cache = CACHE_VAR.setdefault(f'{layer_name}_isochrones', {'index':[],'GeoJSON':{\n",
    "        'type': 'FeatureCollection',\n",
    "        'name': f'{layer_name}_isochrones',\n",
    "        'features':[]\n",
    "        }\n",
    "        })\n",
    "\n",
    "    features_in_cache = []\n",
    "    features_to_fetch = []\n",
    "\n",
    "    for feature in point_feature_collection['features']:\n",
    "        if str(feature['properties']['id']) in layer_cache['index']:\n",
    "            features_in_cache.append(feature)\n",
    "        else:\n",
    "            features_to_fetch.append(feature)\n",
    "\n",
    "    print(f'''Using {len(features_in_cache)} cached isochrones;\n",
    "                Fetching {len(features_to_fetch)} new isochrones''')\n",
    "\n",
    "    if len(features_to_fetch) >0:\n",
    "        new_isochrones = get_isochrones_with_cache({'type': 'FeatureCollection',\n",
    "                                        'name': 'temp',\n",
    "                                        'features': features_to_fetch}, layer_cache)\n",
    "\n",
    "        layer_cache['index'] += new_isochrones['index']\n",
    "        layer_cache['GeoJSON']['features'] += new_isochrones['features']\n",
    "\n",
    "    save_cache(layer_cache, f'{layer_name}_isochrones')\n",
    "\n",
    "    with open('Geospatial_Data/isochrones.geojson', 'w') as file:\n",
    "        json.dump(layer_cache['GeoJSON'], file, indent=2)\n",
    "\n",
    "    return layer_cache['GeoJSON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: http://overpass-api.de/api/interpreter?\n"
     ]
    }
   ],
   "source": [
    "get_market_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geographic_elements = {'type':'FeatureCollection',\n",
    "                  'name':'markets',\n",
    "                  'features':[]}\n",
    "\n",
    "\n",
    "results = call_API_with_cache(url=overpass_url,\n",
    "                              params={'data':overpass_query_markets},\n",
    "                              cache_name='markets', reset_cache=refresh)\n",
    "\n",
    "for element in results['elements']:\n",
    "    if 'tags' in element:\n",
    "        geodict = {'type':'Point'}\n",
    "        propdict = {'id':element['id']}\n",
    "\n",
    "        if element['type'] == 'node' and 'tags' in element:\n",
    "            lon = element['lon']\n",
    "            lat = element['lat']\n",
    "            geodict['coordinates'] = [lon, lat]\n",
    "\n",
    "        elif 'center' in element:\n",
    "            lon = element['center']['lon']\n",
    "            lat = element['center']['lat']\n",
    "            geodict['coordinates'] = [lon, lat]\n",
    "\n",
    "        for key, value in element['tags'].items():\n",
    "            propdict[key] = value\n",
    "\n",
    "        geographic_elements['features'].append({'type':'Feature',\n",
    "                                   'geometry':geodict,\n",
    "                                   'properties':propdict})\n",
    "\n",
    "\n",
    "markets_data = gpd.read_file(json.dumps(geographic_elements))\n",
    "markets_data['wkb_geometry'] = markets_data['geometry'].apply(lambda item: item.wkb)\n",
    "markets_data['addr'] = markets_data.apply(lambda row: f\"{row['addr:housenumber']} {row['addr:street']}, {row['addr:city']}\", axis=1)\n",
    "markets_data['addr'] = markets_data['addr'].apply(lambda x: None if (str(x).find('None') != -1) else x)\n",
    "\n",
    "census_tracts = gpd.read_file('Geospatial_Data/NYC_Tracts.geojson').to_crs('epsg:4326')\n",
    "fields_to_keep = ('id', 'name', 'alt_name', 'addr', 'shop', 'opening_hours', 'phone', 'GEOID','wkb_geometry')\n",
    "\n",
    "markets_data_with_tract = gpd.sjoin(markets_data, census_tracts, how='left', op='intersects')\n",
    "markets_data_with_tract.drop(columns=[column for column in markets_data_with_tract.columns \n",
    "                                      if column not in fields_to_keep], inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
